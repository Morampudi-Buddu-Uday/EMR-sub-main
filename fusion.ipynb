{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BGE-7AFyTnH7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9HZ3cBqyMwuO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from keras.models import model_from_json, load_model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming you have TensorFlow and Keras installed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the speech emotion recognition model\n",
        "with open('/content/drive/MyDrive/model1.json', 'r') as json_file:\n",
        "    speech_model_json = json_file.read()\n",
        "speech_model = model_from_json(speech_model_json)\n",
        "speech_model.load_weights('/content/drive/MyDrive/Emotion_Voice_Detection_Model1 (1).h5')\n",
        "\n",
        "# Load the face emotion recognition model\n",
        "face_model = load_model('/content/drive/MyDrive/face_cnn.h5')\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dN8R3v9Oeyd",
        "outputId": "7212fe30-b651-42c4-9a9f-4e6e77ebb43f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.Feature Level fusion extract\n",
        "####1.Extract Features for Speech and Face Models\n",
        "####2.Combine Features\n",
        "####3.Train a Meta-Classifier\n",
        "####4.Predict and Evaluate"
      ],
      "metadata": {
        "id": "S9z2mDWZUD6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Extract features from audio (same as your original function)\n",
        "def extract_speech_features(file_name, print_flag=False, **kwargs):\n",
        "    mfcc = kwargs.get(\"mfcc\")\n",
        "    chroma = kwargs.get(\"chroma\")\n",
        "    mel = kwargs.get(\"mel\")\n",
        "    contrast = kwargs.get(\"contrast\")\n",
        "    tonnetz = kwargs.get(\"tonnetz\")\n",
        "\n",
        "    with sf.SoundFile(file_name) as sound_file:\n",
        "        X = sound_file.read(dtype=\"float32\")\n",
        "        if X.ndim >= 2:\n",
        "            X = np.mean(X, 1)\n",
        "        sample_rate = sound_file.samplerate\n",
        "        result = np.array([])\n",
        "        if chroma or contrast:\n",
        "            stft = np.abs(librosa.stft(X))\n",
        "        if mfcc:\n",
        "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "            if print_flag:\n",
        "                print(f\"MFCC shape: {mfccs.shape}\")\n",
        "            result = np.hstack((result, mfccs))\n",
        "        if chroma:\n",
        "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "            result = np.hstack((result, chroma))\n",
        "        if mel:\n",
        "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T, axis=0)\n",
        "            result = np.hstack((result, mel))\n",
        "        if contrast:\n",
        "            contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
        "            result = np.hstack((result, contrast))\n",
        "        if tonnetz:\n",
        "            tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T, axis=0)\n",
        "            result = np.hstack((result, tonnetz))\n",
        "    return result\n",
        "\n",
        "# Extract features from face image using face model\n",
        "def extract_face_features(image_file, face_model):\n",
        "    image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
        "    image = cv2.resize(image, (128, 128))  # Adjusted based on your input size\n",
        "    image = np.expand_dims(image, axis=-1)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    face_features = face_model.predict(image)\n",
        "    return face_features.flatten()\n",
        "\n",
        "# Load pre-trained face model\n",
        "face_model = load_model('/content/drive/MyDrive/face_cnn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWoPmLOuOxZG",
        "outputId": "d02b018e-83b5-433e-ce1a-ca036b26f82d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature-level fusion Logic\n"
      ],
      "metadata": {
        "id": "CvA1Fod0TR3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_level_fusion(audio_file, image_file, speech_model, face_model):\n",
        "    speech_features = extract_speech_features(audio_file, mfcc=True)\n",
        "    face_features = extract_face_features(image_file, face_model)\n",
        "\n",
        "    # Combine the features by concatenating them\n",
        "    fused_features = np.hstack((speech_features, face_features))\n",
        "    return fused_features\n"
      ],
      "metadata": {
        "id": "qxM7rsE9RdLu"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Meta-Classifier Training"
      ],
      "metadata": {
        "id": "jUkLbYdcWl2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_face_features(image_file, face_model):\n",
        "    image = cv2.imread(image_file, cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (128, 128))  # Resize to match model input\n",
        "    image = image.astype('float32') / 255.0\n",
        "\n",
        "    if image.shape[-1] == 1:\n",
        "        image = np.repeat(image, 3, axis=-1)  # Convert grayscale to RGB\n",
        "\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    face_features = face_model.predict(image)\n",
        "    return face_features.flatten()"
      ],
      "metadata": {
        "id": "dYPVgivpp8Qb"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_face_emotion(image_file, face_model):\n",
        "    face_features = extract_face_features(image_file, face_model)\n",
        "    if face_features.size == 0:\n",
        "        return None\n",
        "    predicted_face_emotion = np.argmax(face_features)\n",
        "    return predicted_face_emotion\n",
        "\n",
        "def predict_speech_emotion(audio_file, speech_model):\n",
        "    speech_features = extract_speech_features(audio_file, mfcc=True)\n",
        "    speech_features = speech_features.reshape(1, -1)\n",
        "    predicted_speech_emotion = speech_model.predict(speech_features)\n",
        "    return np.argmax(predicted_speech_emotion)\n",
        "\n",
        "def feature_level_fusion(audio_file, image_file, speech_model, face_model):\n",
        "    speech_emotion = predict_speech_emotion(audio_file, speech_model)\n",
        "    face_emotion = predict_face_emotion(image_file, face_model)\n",
        "\n",
        "    # Ensure the emotions are valid (not None)\n",
        "    if speech_emotion is not None and face_emotion is not None:\n",
        "        return np.concatenate([np.array([speech_emotion]), np.array([face_emotion])])\n",
        "    else:\n",
        "        # Handle cases where one of the features is None\n",
        "        return np.concatenate([np.array([speech_emotion if speech_emotion is not None else -1]),\n",
        "                                np.array([face_emotion if face_emotion is not None else -1])])\n",
        "\n",
        "# Example data\n",
        "audio_files = ['/content/drive/MyDrive/03-02-05-01-02-02-05.wav',\n",
        "               '/content/drive/MyDrive/03-02-05-02-02-01-05.wav',\n",
        "               '/content/drive/MyDrive/03-02-06-01-01-01-05.wav']\n",
        "image_files = ['/content/drive/MyDrive/front.jpg',\n",
        "               '/content/drive/MyDrive/gowf.jpg',\n",
        "               '/content/drive/MyDrive/satvikf.jpg']\n",
        "labels = ['happy', 'sad', 'neutral']  # Corresponding labels\n",
        "\n",
        "fused_features = []\n",
        "for audio_file, image_file in zip(audio_files, image_files):\n",
        "    fused_features.append(feature_level_fusion(audio_file, image_file, speech_model, face_model))\n",
        "\n",
        "# Convert to numpy array\n",
        "X_fused = np.array(fused_features)\n",
        "\n",
        "# Encode labels into numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"Shape of X_fused:\", X_fused.shape)\n",
        "print(\"Shape of y:\", len(y))\n",
        "print(\"Labels:\", y)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_fused, y, test_size=0.2, random_state=42)\n",
        "\n",
        "meta_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "meta_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = meta_classifier.predict(X_test)\n",
        "\n",
        "unique_y_test = np.unique(y_test)\n",
        "unique_y_pred = np.unique(y_pred)\n",
        "\n",
        "print(\"Unique classes in y_test:\", unique_y_test)\n",
        "print(\"Unique classes in y_pred:\", unique_y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, labels=unique_y_test, target_names=label_encoder.inverse_transform(unique_y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crBtu01LWjdP",
        "outputId": "b8055697-41b3-429d-867b-8baaf5b44fde"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
            "Shape of X_fused: (3, 2)\n",
            "Shape of y: 3\n",
            "Labels: [0 2 1]\n",
            "Unique classes in y_test: [0]\n",
            "Unique classes in y_pred: [2]\n",
            "Accuracy: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       happy       0.00      0.00      0.00       1.0\n",
            "\n",
            "   micro avg       0.00      0.00      0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(Counter(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYMPVVAarjhO",
        "outputId": "dda09716-92e9-49ec-ab4b-4ba03c569806"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 1, 2: 1, 1: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction"
      ],
      "metadata": {
        "id": "XAsxCSEZTKlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(audio_file, image_file, speech_model,face_model):\n",
        "    fused_features = feature_level_fusion(audio_file, image_file, speech_model, face_model)\n",
        "    predicted_emotion = meta_classifier.predict([fused_features])\n",
        "    return predicted_emotion\n",
        "\n",
        "# Example usage\n",
        "audio_file = '/content/drive/MyDrive/03-02-01-01-01-01-05.wav'\n",
        "image_file = '/content/drive/MyDrive/front.jpg'\n",
        "predicted_emotion = predict_emotion(audio_file, image_file, speech_model,face_model)\n",
        "print(\"Predicted Emotion:\", predicted_emotion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btFTuC_ZW_lr",
        "outputId": "b237c3fb-a5ce-41b8-db7f-d798d45ab721"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "Predicted Emotion: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Label-Level Fusion Implementation\n",
        "####1.Predict Labels Separately from Both Models\n",
        "####2.Combine Labels Using a Voting Mechanism\n",
        "####3.Predict Final Label"
      ],
      "metadata": {
        "id": "hZenjb5AS-wL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Label-level fusion that takes input from drive mounted"
      ],
      "metadata": {
        "id": "QaFMhVxdUZ7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have separate models for audio and face:\n",
        "def predict_speech_emotion(audio_file):\n",
        "    speech_features = extract_speech_features(audio_file, mfcc=True)\n",
        "    speech_features = speech_features.reshape(1, -1)\n",
        "    predicted_speech_emotion = speech_model.predict(speech_features)\n",
        "    return predicted_speech_emotion\n",
        "\n",
        "def predict_face_emotion(image_file, face_model):\n",
        "    face_features = extract_face_features(image_file, face_model)\n",
        "    predicted_face_emotion = np.argmax(face_features)\n",
        "    return predicted_face_emotion\n"
      ],
      "metadata": {
        "id": "DolwNIzUXEfC"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_face_features(image_file, face_model):\n",
        "    image = cv2.imread(image_file, cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (128, 128))\n",
        "    image = image.astype('float32') / 255.0\n",
        "\n",
        "    if image.shape[-1] == 1:\n",
        "        image = np.repeat(image, 3, axis=-1)\n",
        "\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    face_features = face_model.predict(image)\n",
        "    return face_features.flatten()\n",
        "\n",
        "def predict_speech_emotion(audio_file, speech_model):\n",
        "    speech_features = extract_speech_features(audio_file, mfcc=True)\n",
        "    speech_features = speech_features.reshape(1, -1)\n",
        "    predicted_speech_emotion = speech_model.predict(speech_features)\n",
        "    return np.argmax(predicted_speech_emotion)\n",
        "\n",
        "def predict_face_emotion(image_file, face_model):\n",
        "    face_features = extract_face_features(image_file, face_model)\n",
        "    if face_features.size == 0:\n",
        "        return \"Unknown\"  # Or any default value you prefer\n",
        "    predicted_face_emotion = np.argmax(face_features)\n",
        "    return predicted_face_emotion\n",
        "\n",
        "def label_level_fusion(audio_file, image_file, speech_model, face_model):\n",
        "    speech_emotion = predict_speech_emotion(audio_file, speech_model)\n",
        "    face_emotion = predict_face_emotion(image_file, face_model)\n",
        "\n",
        "    # Debugging: Print the emotions\n",
        "    print(f\"Speech Emotion: {speech_emotion}, Face Emotion: {face_emotion}\")\n",
        "\n",
        "    # Voting mechanism: Majority vote or weighted voting\n",
        "    emotions = [speech_emotion, face_emotion]\n",
        "\n",
        "    # Handle the case where one of the predictions might be \"Unknown\"\n",
        "    if \"Unknown\" in emotions:\n",
        "        final_emotion = speech_emotion if face_emotion == \"Unknown\" else face_emotion\n",
        "    else:\n",
        "        final_emotion_result = mode(emotions)\n",
        "        final_emotion = final_emotion_result.mode  # This is already the scalar mode value\n",
        "\n",
        "    return final_emotion\n",
        "\n",
        "# Example usage\n",
        "audio_file = '/content/drive/MyDrive/03-02-01-01-01-01-05.wav'\n",
        "image_file = '/content/drive/MyDrive/front.jpg'\n",
        "\n",
        "\n",
        "final_emotion = label_level_fusion(audio_file, image_file, speech_model, face_model)\n",
        "print(\"Final Predicted Emotion:\", final_emotion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7hHopCDSOCK",
        "outputId": "b552326f-80df-4977-aefe-0bc4ab3ee03b"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "Speech Emotion: 2, Face Emotion: 0\n",
            "Final Predicted Emotion: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "MMzpzk5EbU6w",
        "outputId": "153c170a-b8c5-43ab-d128-7c3e19651ea9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-a250555c70e9>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/03-02-01-01-01-01-05.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/front.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfinal_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_level_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Predicted Emotion:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_emotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-a250555c70e9>\u001b[0m in \u001b[0;36mlabel_level_fusion\u001b[0;34m(audio_file, image_file, speech_model, face_model)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Voting mechanism: Majority vote or weighted voting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfinal_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspeech_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_emotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_emotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py\u001b[0m in \u001b[0;36maxis_nan_policy_wrapper\u001b[0;34m(_no_deco, *args, **kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Extract the things we need here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if something is missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 samples = [np.atleast_1d(kwds.pop(param))\n\u001b[0m\u001b[1;32m    459\u001b[0m                            for param in (params[:n_samp] + kwd_samp)]\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# let the function raise the right error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Extract the things we need here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if something is missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 samples = [np.atleast_1d(kwds.pop(param))\n\u001b[0m\u001b[1;32m    459\u001b[0m                            for param in (params[:n_samp] + kwd_samp)]\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# let the function raise the right error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Dummy functions to simulate predictions\n",
        "def predict_speech_emotion(audio_features):\n",
        "    # Randomly simulate a predicted emotion from the audio features\n",
        "    emotions = ['neutral', 'happy', 'sad', 'angry']\n",
        "    return np.random.choice(emotions)\n",
        "\n",
        "def predict_face_emotion(image_data, face_model):\n",
        "    # Randomly simulate a predicted emotion from the image data\n",
        "    emotions = ['neutral', 'happy', 'sad', 'angry']\n",
        "    return np.random.choice(emotions)\n",
        "\n",
        "# Label-level fusion function\n",
        "def label_level_fusion(audio_features, image_data, speech_model=None, face_model=None):\n",
        "    speech_emotion = predict_speech_emotion(audio_features)\n",
        "    face_emotion = predict_face_emotion(image_data, face_model)\n",
        "\n",
        "    # Use Counter to find the most common emotion (majority voting)\n",
        "    emotion_counter = Counter([speech_emotion, face_emotion])\n",
        "    final_emotion = emotion_counter.most_common(1)[0][0]\n",
        "    return final_emotion\n",
        "\n",
        "# Simulate random audio features and image data\n",
        "audio_features = np.random.rand(100)  # Simulate a 100-dimensional audio feature vector\n",
        "image_data = np.random.rand(128, 128, 3)  # Simulate a 128x128 RGB image\n",
        "\n",
        "# Run label-level fusion\n",
        "final_emotion = label_level_fusion(audio_features, image_data, None, face_model=None)\n",
        "print(\"Final Predicted Emotion:\", final_emotion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFh-Kh3nUgMK",
        "outputId": "fc2551f6-4a4a-40f4-960f-af90c4ace19c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Predicted Emotion: neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAxvoXoTUg0x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}